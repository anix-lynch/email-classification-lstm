---
title: "How to optimize query in RAGs ğŸš€"
datePublished: Wed Jan 08 2025 07:38:57 GMT+0000 (Coordinated Universal Time)
cuid: cm5nl91h5000q09l645yd8jkk
slug: how-to-optimize-query-in-rags
tags: indexing, llm, rag, pre-retrieval-query, multi-query

---

# **What Is Pre-Retrieval Query Optimization?** ğŸš€

Pre-retrieval query optimization is like **training your research assistant** before sending them into a library ğŸ“š. It ensures they know **exactly what to look for** to retrieve the **most relevant data**â€”no wasted time, no irrelevant books!

---

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736321530843/208549b8-7606-44b8-9676-5f4d37edc503.png align="center")

### **Why Does It Matter?** ğŸ¤”

1. **Improved Precision ğŸ¯**
    
    * Focuses the search to retrieve **relevant documents only**.
        
    * Example: Instead of searching for *â€œmachine learning,â€* we refine the query to *â€œapplications of machine learning in healthcare.â€*
        
2. **Enhanced Relevance ğŸ”**
    
    * Handles **multi-intent queries** by **splitting questions** into sub-queries.
        
    * Example:  
        Query: *"Explain AI and its impact on climate change."*
        
        * **Split Queries:**
            
            * *â€œWhat is AI?â€*
                
            * *â€œHow does AI impact climate change?â€*
                
3. **Reduces Ambiguity ğŸ§©**
    
    * Fixes **vague questions** using techniques like:
        
        * **Step-Back Prompting:** Clarifies intent with follow-up prompts.
            
        * **HyDE (Hypothetical Document Embeddings):** Predicts what the **ideal document** should look like.
            
    * Example: Query *â€œgrowthâ€* â†’ Clarify whether it's *economic growth* or *population growth*.
        
4. **Leverages External Knowledge ğŸŒ**
    
    * Routes queries to **domain-specific databases** or **external APIs** based on context.
        
    * Example: If the query involves **medical data**, it routes to **PubMed** instead of general sources.
        
5. **Foundation for Accurate Outputs ğŸ“‘**
    
    * Prepares the LLM with **clean, focused data**, avoiding **hallucinations** or irrelevant results.
        
    * Example: Query: *â€œEffects of meditation on stress levels.â€*
        
        * Finds **research-backed evidence** rather than **blog opinions**.
            

---

### **Techniques for Optimization** ğŸ› ï¸

| **Technique** | **Purpose** | **Example** |
| --- | --- | --- |
| **Multi-Query Retrieval ğŸ§µ** | Splits queries into **sub-queries** to cover multiple angles. | *â€œImpact of AI on climate changeâ€ â†’ â€œWhat is AI?â€ + â€œAI effects on climate.â€* |
| **Decomposition ğŸ”—** | Breaks down **complex queries** into manageable parts. | *â€œBenefits and risks of AIâ€ â†’ Query benefits and risks separately.* |
| **Step-Back Prompting ğŸ§‘â€ğŸ«** | Adds context before query execution to **clarify intent.** | Query: \*â€œHow does it work?â€ â†’ Step back: *â€œWhat does â€˜itâ€™ refer to?â€* |
| **HyDE (Hypothetical Embeddings) ğŸ“** | Predicts **ideal documents** for embedding before retrieval. | Query: *â€œEconomic impact of AIâ€ â†’ Embed AI papers related to economy.* |
| **Semantic Routing ğŸš¦** | Uses **classifiers** to route queries to **specific knowledge bases.** | Query: *â€œHeart disease diagnosisâ€ â†’ Route to PubMed, not Wikipedia.* |

---

### **Real-World Use Case ğŸŒ**

#### Scenario: **Financial Data Analysis** ğŸ“ˆ

**Query**: *â€œHow did Teslaâ€™s stock price react to Elon Muskâ€™s announcements?â€*

**Optimization Steps:**

1. **Multi-Query Retrieval** â†’ Break into:
    
    * *â€œTesla stock price history.â€*
        
    * *â€œTimeline of Elon Muskâ€™s announcements.â€*
        
2. **Semantic Routing** â†’ Route stock prices to **Yahoo Finance API** and announcements to **News APIs.**
    
3. **HyDE** â†’ Generate hypothetical documents about **price trends after major news** to refine matching.
    
4. **Final Output** â†’ Retrieves charts + commentary + summaries.
    

---

### **Why Is It Better?** ğŸš€

Without optimization: **Scattered results**, irrelevant articles, and hallucinated answers.  
With optimization: **Laser-focused outputs**, clear context, and factually accurate insights.

---

# **Multi-Query Techniques for Complex Information Retrieval** ğŸ”

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736321795607/30b49ca0-5f8f-46d6-aabd-49b5af433bcd.png align="center")

#### **What Is Multi-Query?**

Multi-query is a **query expansion technique** in RAG systems that creates **multiple reformulations** of the userâ€™s question to enhance document retrieval. This improves relevance by considering **different perspectives** of the query.

---

### **Why Use Multi-Query?** ğŸ¤–

1. **Better Coverage of Information ğŸ› ï¸**
    
    * Captures **synonyms, variations, and related concepts** of the query.
        
    * Example:
        
        * Original query: *â€œWhat is LangSmith?â€*
            
        * Expanded queries:
            
            * *â€œDefine LangSmith.â€*
                
            * *â€œPurpose of LangSmith.â€*
                
            * *â€œUse cases of LangSmith.â€*
                
2. **Improves Retrieval Accuracy ğŸ¯**
    
    * Reduces missed results due to **poor keyword matching**.
        
    * Broader coverage ensures **no relevant documents are left out**.
        
3. **Ideal for Ambiguous Queries ğŸŒ**
    
    * Handles **multi-intent queries** by splitting them into **sub-questions**.
        
    * Example:
        
        * Query: *â€œExplain AI and its impact on climate change.â€*
            
        * Split into:
            
            * *â€œWhat is AI?â€*
                
            * *â€œHow does AI affect the environment?â€*
                

---

### **Step-by-Step Implementation** ğŸš€

#### **1\. Import Necessary Modules**

```python
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import StrOutputParser
from langchain.chains import LLMChain
from langchain.llms import OpenAI
```

---

#### **2\. Set Up API Keys ğŸ”‘**

```python
import os
os.environ["LANGCHAIN_API_KEY"] = "your_langchain_api_key"
os.environ["OPENAI_API_KEY"] = "your_openai_api_key"
```

---

#### **3\. Prepare and Split Data ğŸ“„**

```python
from langchain.document_loaders import TextLoader

# Load and split documents
loader = TextLoader("example.txt")
documents = loader.load()

splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split_documents(documents)
```

---

#### **4\. Index Documents with Vector Store ğŸ“š**

```python
from langchain.embeddings.openai import OpenAIEmbeddings

# Generate embeddings and store in Chroma
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings)
retriever = vectorstore.as_retriever()
```

---

#### **5\. Generate Multi-Query Variations ğŸ§ **

```python
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Multi-query prompt template
template = """
Generate three variations of the following query to capture different aspects:
Query: {query}

Variations:
"""
prompt = ChatPromptTemplate.from_template(template)

# LLM Chain for generating query variations
llm = OpenAI(temperature=0)  # Lower temperature ensures predictable output
query_chain = LLMChain(llm=llm, prompt=prompt)

# Generate variations
query = "What is LangSmith and why do we need it?"
variations = query_chain.run({"query": query})
print(variations)
```

---

#### **6\. Retrieve Documents Using Multi-Query ğŸ”„**

```python
unique_docs = set()  # Store unique results

for variation in variations.splitlines():
    docs = retriever.get_relevant_documents(variation)  # Fetch relevant docs
    unique_docs.update(docs)  # Add to the unique set
```

---

#### **7\. Run the RAG Model ğŸ“**

```python
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain

# Prompt template for final answer generation
rag_prompt = """
Use the context below to answer the question. Be concise and limit to 3 sentences:
Context: {context}

Question: {question}

Answer:
"""
prompt = ChatPromptTemplate.from_template(rag_prompt)

# Final RAG Chain
llm = OpenAI(temperature=0)
rag_chain = LLMChain(llm=llm, prompt=prompt)

# Combine retrieved context
context = "\n".join([doc.page_content for doc in unique_docs])

# Generate final answer
response = rag_chain.run({"context": context, "question": query})
print(response)
```

---

### **Key Features of Multi-Query Implementation**

1. **Query Diversification** ğŸŒŸ
    
    * Automatically generates **multiple reformulations** of the query.
        
2. **Parallel Retrieval** âš¡
    
    * Runs **parallel searches** to fetch broader document sets.
        
3. **Duplicate Removal** ğŸ§¹
    
    * Ensures retrieved results are **unique** before generating the response.
        
4. **Dynamic Query Expansion** ğŸ§ 
    
    * Captures **synonyms, related terms, and perspectives** automatically.
        
5. **Configurable LLM Chains** ğŸ”—
    
    * Uses **ChatOpenAI** with templates for customization.
        

---

### **Sample Output** ğŸ“

**Input Query:**  
*â€œWhat is LangSmith and why do we need it?â€*

**Generated Queries:**

1. *â€œWhat does LangSmith do?â€*
    
2. *â€œExplain the features of LangSmith.â€*
    
3. *â€œWhy is LangSmith useful in RAG systems?â€*
    

**Final Answer:**  
*LangSmith is a tool for building and optimizing RAG pipelines. It simplifies query management, improves search accuracy, and supports scalable document retrieval. Thanks for asking!* âœ…

---

### **Pro Tips** ğŸŒŸ

* **Test with Ambiguous Queries:** Experiment with **multi-intent questions** for better outputs.
    
* **Scale Vector Stores:** Replace **Chroma** with **Qdrant** for **high-performance storage** if needed.
    
* **Monitor Query Performance:** Use tools like **LangSmith** to analyze how queries are processed.
    
* **Fine-Tune Prompt Templates:** Adjust prompts to fit **domain-specific requirements**.
    

---