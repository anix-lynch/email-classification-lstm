---
title: "Data collection for RAG(wikipedia)"
seoTitle: "Data collection for RAG(wikipedia)"
seoDescription: "Data collection for RAG(wikipedia)"
datePublished: Thu Nov 28 2024 09:45:36 GMT+0000 (Coordinated Universal Time)
cuid: cm414pzk2000u09l12al4gt42
slug: data-collection-for-ragwikipedia
tags: rag

---

### **Part 1: Environment Setup**

#### **Step 1: Install Required Packages**

```python
!pip install beautifulsoup4==4.12.3
!pip install requests==2.31.0
```

**Explanation**:

* The packages **BeautifulSoup4** and **requests** are installed. These will be used for **web scraping** (fetching data from Wikipedia) and **HTML parsing**.
    

**Output**:

* Packages will be installed if not already present.
    

---

### **Part 2: Importing Required Libraries**

```python
import requests
from bs4 import BeautifulSoup
import re
```

**Explanation**:

* Importing the necessary libraries:
    
    * `requests`: For making HTTP requests to fetch the HTML content from URLs.
        
    * `BeautifulSoup`: For parsing and navigating the HTML content.
        
    * `re`: For regular expressions to clean and process text.
        

---

### **Part 3: Define URLs for Wikipedia Articles**

```python
# URLs of the Wikipedia articles
urls = [
    "https://en.wikipedia.org/wiki/Space_exploration",
    "https://en.wikipedia.org/wiki/Apollo_program",
    "https://en.wikipedia.org/wiki/Hubble_Space_Telescope",
    "https://en.wikipedia.org/wiki/Mars_rover",  # Corrected link
    "https://en.wikipedia.org/wiki/International_Space_Station",
    "https://en.wikipedia.org/wiki/SpaceX",
    "https://en.wikipedia.org/wiki/Juno_(spacecraft)",
    "https://en.wikipedia.org/wiki/Voyager_program",
    "https://en.wikipedia.org/wiki/Galileo_(spacecraft)",
    "https://en.wikipedia.org/wiki/Kepler_Space_Telescope"
]
```

**Explanation**:

* A list of URLs pointing to **Wikipedia articles** related to space exploration topics.
    

**Output**:

* This step doesn’t produce immediate output but sets up the URLs to scrape.
    

---

### **Part 4: Scraping and Cleaning Data**

#### **Step 4: Clean Text Function**

```python
def clean_text(content):
    # Remove references that usually appear as [1], [2], etc.
    content = re.sub(r'\[\d+\]', '', content)
    return content
```

**Explanation**:

* `clean_text` removes reference markers like `[1]`, `[2]`, etc., which are often seen in Wikipedia articles.
    

**Output**:

* This function will return the cleaned content without reference numbers.
    

---

#### **Step 5: Fetch and Clean Content**

```python
def fetch_and_clean(url):
    # Fetch the content of the URL
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find the main content of the article, ignoring side boxes and headers
    content = soup.find('div', {'class': 'mw-parser-output'})

    # Remove the bibliography section which generally follows a header like "References", "Bibliography"
    for section_title in ['References', 'Bibliography', 'External links', 'See also']:
        section = content.find('span', id=section_title)
        if section:
            # Remove all content from this section to the end of the document
            for sib in section.parent.find_next_siblings():
                sib.decompose()
            section.parent.decompose()

    # Extract and clean the text
    text = content.get_text(separator=' ', strip=True)
    text = clean_text(text)
    return text
```

**Explanation**:

* `fetch_and_clean`:
    
    * Makes an HTTP request to fetch content from the URL.
        
    * Uses **BeautifulSoup** to parse and find the main content of the article.
        
    * Removes unnecessary sections like **References**, **Bibliography**, **External Links**, etc.
        
    * The article content is cleaned and returned after removing unwanted sections and reference markers.
        

**Output**:

* Returns the **cleaned text** for each article.
    

---

### **Part 5: Saving the Cleaned Data**

#### **Step 6: Save the Cleaned Data to a File**

```python
# File to write the clean text
with open('llm.txt', 'w', encoding='utf-8') as f:
    for url in urls:
        text = fetch_and_clean(url)  # Fetch and clean the content from the URL
        f.write(f"Article: {url}\n")
        f.write(text)
        f.write("\n\n")
```

**Explanation**:

* This code loops through all the **URLs**, scrapes, and cleans the content using the `fetch_and_clean` function.
    
* It writes the cleaned content for each article into the `llm.txt` file, with the article's URL as a heading.
    

**Output**:

* The cleaned text from all the articles is saved to the `llm.txt` file, with each article's content separated by newlines.
    

---

### **Summary of Output So Far:**

* The script scrapes data from the provided **Wikipedia URLs**, cleans it by removing references and unwanted sections, and saves the cleaned content into a file called `llm.txt`. Each article’s content is saved with the article's URL as a header.