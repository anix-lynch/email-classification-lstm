---
title: "Machine Learning Technical Q&A"
seoTitle: "Machine Learning Technical Q&A (1/2)"
seoDescription: "Machine Learning Technical Q&A (1/2)"
datePublished: Fri Oct 18 2024 00:37:28 GMT+0000 (Coordinated Universal Time)
cuid: cm2e035y9000009l31ppw2obh
slug: machine-learning-technical-qa
tags: ai, python, data-science, machine-learning, deep-learning

---

**What is the main role of a validation set in machine learning?**  
It is used to fine-tune the model's hyperparameters

**Which term describes a measure of <mark>how well</mark> a machine learning model generalizes to new, unseen data?**  
<mark>Variance</mark>

**What is the main purpose of cross-validation in machine learning?**  
To evaluate a model's performance on unseen data

**Which term refers to the process of modifying model parameters <mark>to reduce errors </mark> on the training data?**  
Optimization

**What is the main difference between a generative model and a discriminative model?**  
Generative models generate new data samples, while discriminative models classify data

**Which step of the machine learning process involves making predictions <mark>using the trained model?</mark>**  
Inference

---

**What is the main purpose of a test set in machine learning?**  
To make predictions on new data

**What is the primary reason for using <mark>feature scaling</mark> in machine learning?**  
To improve the model's accuracy

**Which term refers to the process of dividing a dataset into training and testing sets?**  
Partitioning

**What is the key difference between a hyperparameter and a parameter in a machine learning model?**  
<mark>Parameters are learned from data, while hyperparameters are set before training</mark>

**Which of the following statements about the bias-variance trade-off is true?**  
<mark>Reducing bias often increases variance</mark>

**Which of the following is an example of an unsupervised learning task?**  
Clustering similar images together

**What is the purpose of <mark>a loss function</mark> in machine learning?**  
<mark>To quantify the error between predicted and actual values</mark>

---

**Which phase of the machine learning process involves fine-tuning the model's parameters to achieve better performance?**  
Hyperparameter tuning

**Which type of machine learning algorithm aims to <mark>find correlations between variables?</mark>**  
<mark>Association</mark>

**What is impact of high bias of a model?**  
Imagine you're trying to draw a map of a really complex city, but all you have is a simple sketch of straight roads and boxes for buildings. That's **high bias**—your sketch is so simple that it ignores all the little details of the real city, like curving streets and unique building shapes.

In machine learning, when a model <mark>has </mark> **<mark>high bias</mark>**<mark>, it’s like that simple sketch. The model tries to generalize too much</mark>, assuming everything is straightforward and missing the real complexities of the data. <mark>This results in </mark> **<mark>underfitting</mark>**—the model doesn't capture the important patterns and relationships in the data because it’s trying to fit everything into an overly simple structure.

**What does the term "supervised" in supervised learning refer to?**  
The model uses labeled data for training

**Which term describes the process of updating a model's parameters using the difference between predicted and actual values?**  
Optimization

**What is the primary goal of an unsupervised learning algorithm?**  
To find patterns in data

**Which of the following is a common challenge in machine learning?**  
Overfitting the training data

---

**What is the fundamental building block of a neural network?**  
Neuron

---

**In a neural network, what does <mark>a connection weight </mark> represent?**  
<mark>The strength of the connection</mark> between neurons

---

**Which type of activation function is commonly used in the output layer of a binary classification neural network?**

Sigmoid.

The sigmoid function **squeezes** values between 0 and 1, and that's exactly why it has its advantages and disadvantages:

* **Advantage**: <mark>In tasks like binary classification</mark>, squeezing values between 0 and 1 is useful because it allows us to interpret the output as a probability (e.g., probability of class A vs. class B).
    
* **Disadvantage**: When sigmoid squeezes values, especially very large or very small numbers, the gradients (used to update the model weights) become very small, which **slows down learning**—this is called the **vanishing gradient problem**.
    

So, while sigmoid is helpful for tasks that require outputs between 0 and 1 (like probabilities), it can slow down learning in deeper networks because it "saturates" and reduces the gradient size, making it harder for the model to learn effectively. This is why ReLU, which avoids this squeezing, is often preferred in modern deep learning architectures.

---

**What does the term <mark>"backpropagation"</mark> refer to in the context of neural networks?**  
The process of <mark>adjusting connection weights based on prediction errors</mark>

---

**What is the main advantage of using ReLU (Rectified Linear Unit) activation function in neural networks?**  
It introduces non-linearity without saturation. Think of the ReLU function as a switch. It turns "on" when a value is positive (letting that value pass through), b<mark>ut it stays "off" (outputting zero) when the value is negative.</mark>

The main advantage? **It helps the neural network learn faster** without getting stuck. In simpler terms, it keeps things moving smoothly by only letting the important signals (positive values) through <mark>and ignoring the rest (negative values)</mark>, which helps the network focus on the useful information.

Unlike some other activation functions, ReLU doesn’t slow down the learning by making everything small or stuck (which happens in functions that "saturate"). Some activation functions (like sigmoid or tanh) have outputs that get squeezed into a very narrow range, like between 0 and 1, especially when input values are very large or very small.

This squeezing is called **saturation**. When the outputs are squished like this, the network struggles to learn because the changes in the output are tiny, even if the input changes a lot. This can slow down the learning process because the model doesn’t adjust much during training.

---

**What is the purpose of weight and bias in a neural network?**  
To shift the activation function.

Think of the **bias** as a thermostat in your house.

* The **input** (temperature outside) affects how hot or cold it feels inside.
    
* The **weight** is like the insulation of your house, determining how much the outside temperature impacts the inside.
    
* The **bias** is the thermostat setting, which shifts the comfort level up or down.
    

---

**Which term describes the process of adjusting the learning rate during training to speed up convergence?**

<mark>Learning rate annealing.</mark>

It refers to the process of **gradually reducing** the learning rate during training to help the model converge more efficiently.

---

**What is the main objective of <mark>regularization</mark> techniques in neural networks?**  
<mark>To prevent overfitting</mark>

---

**Which type of neural network layer <mark>connects each neuron to every neuron</mark> in the subsequent layer?**  
<mark>Fully Connected (Dense) Layer</mark>

---

**What is the primary purpose of the <mark>pooling layer </mark> in a convolutional neural network (CNN)?**  
To reduce the dimensionality of the feature map

---

**What is the role of the activation function in a neural network?**  
It introduces non-linearity to the neuron's output. Non-linearity is crucial for capturing complex patterns

---

**Which activation function should be used in the <mark>output layer</mark> of a neural network when performing <mark>multi-class classification?</mark>**  
Softmax.

**ReLU**, **sigmoid**, and **tanh** are not suitable judges for **multi-class classification** because they don't handle multiple classes the same way **softmax** does. Here's why:

* **Sigmoid**: It gives an output between 0 and 1 but treats each output independently, which is useful for **binary classification** (one class or another) or **multi-label classification** (where multiple classes can be true at the same time), but not for choosing one class from many.
    
* **ReLU**: It only outputs positive values (or zero), but it doesn't provide probabilities or handle multiple outputs in a normalized way, so it’s more suitable for hidden layers, not the output layer in multi-class classification.
    
* **Tanh**: Outputs values between -1 and 1, and while it’s useful for controlling the range of outputs, it doesn’t normalize them into probabilities for multiple classes.
    

In **multi-class classification**, you need **softmax**, who evaluates all the classestogether and decides based on probabilities. **Other judges** like **sigmoid, ReLU, and tanh** don’t know how to fairly compare all classes and pick just one winner.

---

**What is the purpose of the loss function in a neural network?**  
To calculate the gradients for backpropagation.

**Backpropagation**: When the network makes a prediction, we calculate how wrong it was (the **loss**). The gradients help us figure out how to **adjust the weights** to reduce the error for the next prediction.

* If the gradient is **positive**, you decrease the weight to reduce the loss.
    
* If the gradient is **negative**, you increase the weight to reduce the loss.
    

Imagine you're shooting basketball hoops, but you miss. <mark>The </mark> **<mark>loss function</mark>** <mark>calculates how far off your shot was. </mark> The **gradient** tells you how much you need to adjust your arm's angle (weights) to get closer to the basket next time.

In summary:

* **Gradient**: Tells you how to change the weights to reduce the loss.
    
* **Backpropagation**: Uses gradients to **update the weights** step by step, reducing the error.
    

---

**Which technique is used to initialize the weights of a neural network to small random values?**  
He initialization.

This particularly suited for networks that use the **ReLU** activation function. It adjusts the random weights based on the size of the previous layer to ensure that the gradients don’t vanish or explode during training

---

**Which of the following is a common optimization algorithm used in training deep neural networks?**  
Stochastic Gradient Descent (SGD)

**Stochastic Gradient Descent (SGD)** is an optimization algorithm used to update the weights of a neural network during training.

* **Goal**: Minimize the loss (error) by adjusting weights.
    
* **Stochastic**: <mark>Instead of using the whole dataset, it updates weights after looking at </mark> **<mark>one or a few data points</mark>** <mark>(randomly selected)</mark> at a time.
    
* **Why**: Makes training faster and <mark>less computationally expensive, especially with large datasets.</mark>
    
* **Process**:
    
    1. Compute the gradient (slope) of the loss for a data point.
        
    2. Update the weights in the direction that **reduces the loss**.
        
    3. Repeat this for many iterations until the model converges.
        

---

**Which term describes the process of gradually reducing the learning rate during training?**  
Learning rate annealing

---

**Which type of activation function is commonly used in the hidden layers of a deep neural network?**  
ReLU (Rectified Linear Unit)

---

**What is the primary disadvantage of using <mark>a very deep</mark> neural network and how to make sure it is not too deep?**  
It increases the risk of <mark>vanishing gradients</mark>.

The primary disadvantage of using a very deep neural network is the increased risk of **vanishing gradients**, which makes it hard for the model to learn and update weights in the earlier layers. In very deep networks, the gradients (slopes) can become extremely small as they are passed back through many layers, leading to almost no weight updates, especially in the early layers.

<mark>Default setup to avoid this:</mark>

* **Number of Layers**: Start with a reasonable number of layers (e.g., 3-5 for simpler problems, and incrementally increase for more complex tasks).
    
* **Activation Functions**: Use **ReLU** activation (or its variants) instead of sigmoid or tanh, as ReLU helps reduce the risk of vanishing gradients.
    
* **Initialization**: Use proper weight initialization methods like **<mark>He initialization</mark>** to avoid starting with weights that can cause gradients to vanish.
    
* **Batch Normalization**: Apply **batch normalization**, which helps stabilize learning by normalizing inputs across layers and can mitigate the vanishing gradient problem.
    

<mark>How to know it's getting too deep:</mark>

* **<mark>Training Loss Stops Decreasing</mark>**: If the model is not learning (loss isn't going down) even though you're increasing the number of layers, this might be a sign.
    
    In this graph, you see that the **training loss** starts decreasing but then flattens after a certain point, indicating the model is no longer learning much.
    
* The **validation loss** decreases at first but starts increasing later, suggesting **overfitting**.
    
    ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729222684304/f553bc0a-2049-4bbc-aec4-35dd19e3779c.png align="center")
    
    **<mark>Gradients Near Zero</mark>**: Monitor the gradients. If they become very small (close to zero), especially in the early layers, the network might be too deep.
    
    In the second graph (right), the **gradient norms** for Layer 2 are very small (close to zero), which suggests **vanishing gradients**, while Layer 1 has normal gradient values.
    
    ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729222703237/d733ecb6-44f7-4818-b50a-ce906b9cac90.png align="center")
    
    **<mark>Overfitting</mark>**: If your model performs well on the training set but poorly on the validation/test set, this can be a sign that the network is unnecessarily deep and complex.
    
* The **training accuracy** keeps improving, but the **validation accuracy** starts to drop after some epochs. This indicates **overfitting**, where the model is performing well on the training set but poorly on unseen data.
    
    ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729222564212/4b87324b-c812-4c4a-8b9e-178ddcf5bfea.png align="center")
    

Start simple, then gradually add layers while monitoring performance to avoid making the network too deep.

---

**Which of the following is a characteristic of the vanishing gradient problem?**  
Difficulties in training deep networks due to small gradient magnitudes

---

**What is the purpose of batch normalization in deep neural networks?**  
<mark>To speed up t</mark>he training process

---

**What does the term "epoch" refer to in the context of neural network training?**  
A single forward and backward pass through the entire dataset.

**Epoch** is like a **rep** in a workout. Just as a rep means completing one full motion of an exercise, an **epoch** means the model has gone through the **entire dataset** once—forward and backward (<mark>including weight updates</mark>)

---

**Which of the following statements about dropout in neural networks is true?**  
Dropout randomly <mark>deactivates neurons</mark> during training.

---

**Which term refers to a technique that helps neural networks learn <mark>hierarchical features</mark> by training on random subsets of the input data?**  
Data augmentation.

**<mark>Hierarchical features</mark>** <mark>refer to </mark> the layers of complexity that a neural network learns, <mark>step by step</mark>, as it processes data. The network starts by recognizing simple patterns (like edges or shapes), then combines them into more complex patterns (like parts of objects), and finally, at the highest level, it recognizes full objects or abstract concepts.

In a neural network:

* Early layers learn **simple features** (like edges in images).
    
* Middle layers combine these into **more complex patterns** (shapes, textures).
    
* Final layers capture **high-level features** (like recognizing a face, object, or scene).
    

---

**Which of the following activation functions is NOT bounded?**  
ReLU (Rectified Linear Unit) It outputs values from **0 to infinity**, meaning it doesn't have an upper limit.

In contrast, activation functions like **sigmoid** and **tanh** are **bounded** because they map their outputs to a fixed range:

* **Sigmoid**: Bounded between 0 and 1.
    
* **Tanh**: Bounded between -1 and 1.
    

ReLU is unbounded on the positive side, which helps during training by avoiding saturation but can also lead to very large activations.

---

**Which type of neural network layer is specifically designed to handle sequential data?**  
Recurrent Layer

---

**What is the key advantage of using a convolutional layer in a neural network for image analysis?**  
It can capture local patterns and spatial hierarchies.

---

**Which optimization algorithm incorporates <mark>a moving average of past gradients </mark> to speed up convergence?**  
Momentum optimization

---

**What is the primary goal of weight regularization in neural networks?**

To prevent overfitting by adding a penalty to large weights. The goal is to make the model more generalizable by keeping the weights small, which helps the model perform better on new, unseen data.

Think of regularization like **keeping things balanced**. If a neural network's weights get too large, it's like a student memorizing answers instead of learning the concepts. <mark>Regularization forces the model to </mark> **<mark>learn patterns</mark>**<mark>, not just memorize the training data.</mark>

Two common types of weight regularization:

* **L2 regularization (Ridge)**: Penalizes large weights by <mark>adding their square to the loss.</mark>
    
* **L1 regularization (Lasso)**: <mark>Adds the absolute value</mark> of weights to the loss, encouraging <mark>sparsity (some weights becoming zero)</mark>.
    

---

**Which activation function should be used in the output layer of a neural network for multi-label classification?**

Sigmoid.  
In **multi-label classification**, a picture can contain **both a cat and a dog**, and the model is expected to predict **both labels**.

For example:

* using **softmax**\- when you ask the model to decide if the picture contains **either** a cat, **or** a dog, **but not both**.
    
* using **sigmoid**\-when you ask the model to predict if the picture contains **both** a cat and a dog. In this case, the model could output:
    
    * Cat: 0.8 (high probability)
        
    * Dog: 0.9 (high probability)
        

---

**Which technique is used to avoid the vanishing gradient problem in deep neural networks?**  
Batch normalization

---

**What is the primary purpose of the pooling layer in a convolutional neural network (CNN)?**  
To reduce the dimensionality of the data

---

**Which type of neural network layer is typically used for dimensionality reduction and downsampling in convolutional neural networks?**  
Pooling Layer

---

**What does the term "weight initialization" refer to in neural networks?**  
The process of setting initial values for connection weights

---

**In a neural network, which of the following terms refers to the error calculated during the forward pass?**  
Loss function error

---

**In deep learning, which technique helps gradients flow smoothly through the network and mitigate the vanishing gradient problem?**  
Batch normalization

---

**Which type of neural network layer is used to merge information from different parts of the input data?**  
Fully Connected (Dense) Layer

**Which term describes the process of adjusting the learning rate based on the <mark>history of gradients</mark> during training?**  
Momentum optimization

---

**Which technique is used to prevent large gradients from causing issues during neural network training?**  
Gradient clipping

---

**In a neural network, which type of layer is responsible for reducing the spatial dimensions of the input data?**  
Pooling Layer

In the context of a neural network, when we say "spatial dimensions," we're talking about the **height and width** of an image or any input data that has a structure (like a grid or layout).

So, a **Pooling Layer** reduces the **height and width** (the spatial dimensions) of the input data while keeping the important information intact. It's like zooming out and keeping only the most important details.

---

**Which type of neural network architecture is specifically designed to handle sequential data while addressing the vanishing gradient problem?**  
Long Short-Term Memory (LSTM)

---

**In which scenario would you use a Gated Recurrent Unit (GRU) instead of a Long Short-Term Memory (LSTM)?**  
When you need a simple recurrent layer

GRUs are like a simplified version of LSTMs, with fewer gates and a more straightforward structure, which makes them:

* **Faster to train** and less computationally expensive.
    
* Suitable when you have less complex data or need quicker results without sacrificing too much performance.
    

---

**What is the primary function of the "gating" mechanisms in Long Short-Term Memory (LSTM) networks?**  
To ensure a smooth gradient flow during backpropagation

---

**Which neural network architecture is suitable for tasks that involve <mark>predicting the next item in a sequence based on the previous items</mark>?**  
Long Short-Term Memory (LSTM)

---

**Which type of neural network architecture uses attention mechanisms to focus on different parts of the input sequence while making predictions?**  
Transformer

---

**What is the main advantage of the Transformer architecture compared to RNNs and LSTMs?**

**Transformers** over RNNs and LSTMs is that they can process **entire input sequences at once**, instead of step-by-step like RNNs or LSTMs. This allows them to handle **long or variable-length sequences** more efficiently and capture patterns in the data better.

---

**In the context of neural network architectures, what does "self-attention" refer to?**  
**Self-attention** allows the model to **pay attention to elements that are not adjacent** in the sequence

---

**What is the primary advantage of using the transformer-based architecture for natural language processing tasks?**  
It eliminates the need for recurrence.

**Recurrence** in neural networks refers to the process where models like RNNs (Recurrent Neural Networks) process sequences **one step at a time**, maintaining an internal memory of previous steps.

**transformers** is that they eliminate this step-by-step dependency, allowing the model to process all elements in a sequence **simultaneously**.

**What is the role of the encoder in a sequence-to-sequence neural network architecture?**  
To compress the input sequence into a fixed-size vector

---

**Which neural network architecture is designed to learn a compact representation of input data in an unsupervised manner?**  
Autoencoder

---

**In an autoencoder, which part of the network is responsible for reconstructing the input data?**  
Decoder

---

**Which neural network architecture <mark>combines both supervised and unsupervised </mark> learning to improve the model’s performance?**  
Transfer Learning

---

**Which technique is commonly used in transfer learning to adapt a pre-trained model to a new task?**  
Fine-tuning

---

**Which neural network architecture is designed to generate <mark>new</mark> content that is similar to a given input?**  
Generative Adversarial Network (GAN)

---

**In a GAN, what is the role of the <mark>discriminator</mark>?**  
<mark>To classify input data as real or fake</mark>

---

**Which neural network architecture is commonly used for modeling sequences with variable lengths and generating captions for images?**  
Recurrent Neural Network (RNN)

---

**What is the main advantage of using a <mark>bidirectional recurrent neural network (BiRNN)</mark>?**  
It can process sequences in both forward and backward directions.

---

**Which neural network architecture is commonly used for generating human-like text based on a given prompt?**  
Transformer

---

**In a transfer learning scenario, which layers of a pre-trained model are typically fine-tuned on a new task?**  
All layers except the output layer

---

**Which neural network architecture is commonly used for tasks such as image captioning and machine translation?**  
Transformer

---

**Which optimization algorithm uses a learning rate that changes during training based on the performance of the model?**  
Adam optimizer

---

**Which term refers to the <mark>rate at which the weights of a neural network are updated </mark> during training?**  
Learning rate

---

**What is the purpose of the learning rate in neural network training?**  
To control the speed of convergence during training

---

**What is the main challenge of setting the learning rate too high during neural network training?**  
Oscillation during training

---

**Which technique is used to prevent the model from updating the weights and biases too drastically during training?**  
Gradient clipping

---

**In the context of neural network training, what is "early stopping"?**  
Stopping the training process <mark>before reaching the maximum number of epochs</mark>

---

**What is the main purpose of the concept of "epochs" in neural network training?**  
To specify <mark>the number of times the entire dataset is passed through the network</mark>

---

**Which regularization technique involves adding a term to the loss function that penalizes large weights?**  
L2 regularization

---

**Which type of weight regularization involves adding the absolute values of the weights to the loss function?**  
L1 regularization

---

**Which technique is used to adjust the learning rate during training to improve convergence?**  
Learning rate scheduling

---

**In the context of neural network training, what does "batch size" refer to?**  
<mark>The number of training examples</mark> processed in a single iteration

---

**Which technique involves dividing the dataset into <mark>smaller</mark> batches and updating the model's parameters after each batch during training?**  
Stochastic Gradient Descent (SGD)

---

**Which type of gradient descent uses the <mark>entire </mark> dataset to compute the gradient of the loss function in each iteration?**  
Batch Gradient Descent

---

**What is the advantage of using mini-batch gradient descent over batch gradient descent?**  
Mini-batch gradient descent is less memory-intensive

---

**What is the main disadvantage of using mini-batch gradient descent?**  
It can get stuck in local minima

Imagine you're hiking in a mountainous area, and you want to reach the **lowest point** (global minimum). You might get stuck <mark>in a small dip (</mark>**<mark>local minima</mark>**<mark>)</mark> and think you've found the lowest spot, but there could be a much deeper valley elsewhere.

In **mini-batch gradient descent**, the updates are based on small batches of data, which can cause noisy updates, sometimes leading the model to settle in **local minima** instead of finding the **global minimum**, where the error is truly minimized.

---

**What is the main advantage of ensemble learning in neural networks?**  
It can improve the model's performance and generalization

---

**In the context of neural network training, what is the purpose of the "plateau" in a learning rate schedule?**  
To adjust the learning rate based on the validation performance

---

**Which technique involves <mark>adding noise </mark> to the weights of a neural network to improve its generalization?**  
Weight regularization

---

**What is the main disadvantage of using a high dropout rate during neural network training?**  
Slower convergence

---

**In the context of neural network training, what is the purpose of the "patience" parameter in early stopping?**  
To specify the number of epochs without improvement before stopping

**Epoch** is like an **exercise rep**, the **patience** parameter would be like how long you’re willing to keep exercising without seeing any muscle growth or improvement before you decide to **stop the workout plan**. **Overfitting** would be like **training too much** without allowing your body to recover, leading to negative effects rather than improvement.

---

**Which technique involves training a neural network using examples from previous time steps to improve its performance on sequence data?**  
**Recurrent Layers** (such as RNN, LSTM, or GRU).

---

**In the context of neural network training, which technique is used to train a model to predict the next value in a sequence given previous values?**  
Sequence-to-Sequence Learning

---

**Which term describes the phenomenon where a neural network performs well on both the training data and new, unseen data?**  
Generalization

---

**What is the primary goal of using a test set for evaluating a machine learning model?**  
To measure the model's performance on completely new data

---

**What is the main disadvantage of an underfitted machine learning model?**  
It performs poorly on new data.

---

**In the context of model evaluation, what does the "bias-variance trade-off" refer to?**  
The trade-off between the model's complexity and its performance on new data.

The **bias-variance trade-off** refers to the balance between a model's complexity and its ability to generalize to new data.

* **<mark>High bias</mark>** <mark>(underfitting) means the model is too simple and doesn't capture the underlying patterns</mark> in the data.
    
* **<mark>High variance</mark>** <mark>(overfitting)</mark> means the model is too complex and fits the training data too closely, <mark>failing to generalize </mark> to new data.
    

Imagine you're learning to shoot a basketball:

* **High bias (underfitting)**: You’re too far away from the basket, and your shots consistently miss the target by a wide margin. You’re **oversimplifying** by not adjusting your aim or force, so you keep missing the hoop. The shots don’t capture the complexity of the task (like not capturing the underlying patterns in the data).
    
* **High variance (overfitting)**: Now, you're standing right next to the basket, and while your shots always hit the hoop, you’ve learned **too precisely** for this one spot. When you step back or change the court, your accuracy falls apart. You’ve **over-adapted** to one specific scenario (like fitting too well to the training data), but you can’t generalize to different distances (new data).
    
* **The balance**: You need to stand at a reasonable distance where your shooting technique is good enough to hit the basket in different locations (generalizing well to new data) but not so rigid that it only works in one spot (overfitting) or so basic that you can’t hit the hoop at all (underfitting).
    

So, in summary:

* **High bias**: Too far from the basket (shots are always wrong).
    
* **High variance**: Too close, over-adapted (only works in one position).
    
* **Balanced**: Adjust your distance and technique to hit the target from different spots (good generalization).
    

This analogy helps you see the **balance** needed in the bias-variance trade-off.

---

**Which evaluation metric is commonly used for classification tasks to measure the proportion of true positive predictions <mark>out of all positive samples</mark>?**  
Recall

---

**Which evaluation metric considers the ratio of true positive predictions to the sum of true positives and false positives?**  
Precision

---

**What is the F1-score a harmonic <mark>mean</mark> of?**  
Precision and recall

---

**Which evaluation metric provides a balance between precision and recall for imbalanced datasets?**  
F1-score

---

**In the context of classification, what is the "confusion matrix"?**  
A matrix that shows the performance of a model's predictions compared to the true labels

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729240966463/1b7e25d2-ae45-474d-85f3-773f9ef9b508.png align="center")

Certainly! Here's a breakdown of the **confusion matrix** and what each part represents:

1. **True Positives (TP) – 50**:
    
    * These are cases where the model <mark>correctly predicted "Cat."</mark>
        
    * Example: The model sees a picture of a cat and correctly identifies it as a cat.
        
2. **False Positives (FP) – 10**:
    
    * These are cases where the model incorrectly predicted "Cat," but it was actually a "Non-Cat."
        
    * Example: The model sees a picture of a dog and **incorrectly** predicts it as a cat. This is a **<mark>false alarm</mark>**.
        
3. **False Negatives (FN) – 5**:
    
    * These are cases where the model predicted "Non-Cat," but it was actually a "Cat."
        
    * Example: The model sees a picture of a cat and **fails** to recognize it, predicting it as a non-cat. This is a **<mark>missed detection</mark>**.
        
4. **True Negatives (TN) – 35**:
    
    * These are cases where the model <mark>correctly predicted "Non-Cat.</mark>"
        
    * Example: The model sees a picture of a dog and correctly identifies it as a non-cat
        

Overall, the confusion matrix helps evaluate how well the model is performing by showing where it gets things right (TP, TN) and where it makes mistakes (FP, FN).

---

**Which term refers to the percentage of correctly predicted instances out of the total instances?**  
Accuracy

---

**In the context of regression, which evaluation metric measures the a<mark>verage squared difference </mark> between predicted and actual values?**  
<mark>Root </mark> Mean Squared Error (RMSE)

---

**Which evaluation metric for regression tasks gives <mark>equal weight</mark> to all errors, regardless of their magnitude?**  
Mean Absolute Error (MAE)

---

**Which evaluation metric is used to measure the goodness of fit for a regression model, taking values between 0 and 1?**  
R-squared  
**R-squared** tells you how well the model's predictions match the actual data. <mark>The closer it is to </mark> **<mark>1</mark>**<mark>, the better the model fits the data.</mark>

* **1** means the model perfectly predicts the data (perfect fit).
    
* **0** means the model does not predict the data well at all.
    

---

**What is the range of values for the R-squared metric?**  
0 to 1

---

**Which evaluation metric provides a measure of a model's performance across different probability thresholds in binary classification?**  
ROC-AUC

**AUC** (Area Under the Curve) measures how well the model distinguishes between the two classes (cats and non-cats) across these thresholds. An AUC of **1** means perfect classification, <mark>while </mark> **<mark>0.5</mark>** <mark>means the model is no better than random guessing.</mark>

---

**Which term describes the plot that illustrates the trade-off between true positive rate and false positive rate across different probability thresholds in binary classification?**  
ROC curve.

---

**Which evaluation metric can be used to <mark>compare models across various probability thresholds</mark> in binary classification?**  
ROC-AUC

How does changing the threshold affect the confusion matrix?

1. **Increase the threshold (e.g., 0.7)**:
    
    * The model becomes **more strict** about predicting "cat." Now, it requires a higher probability (≥ 0.7) to classify an image as a "cat."
        
    * As a result: **TP (True Positives)**: Fewer actual cats may meet this higher threshold, so the **TP** could **decrease**.
        
2. **Decrease the threshold (e.g., 0.3)**:
    
    * The model becomes **less strict**, requiring a lower probability (≥ 0.3) to classify an image as a "cat."
        
    * As a result: **TP (True Positives)**: More actual cats may be classified correctly, so **TP** could **increase**.
        

---

**Which cross-validation technique involves splitting the dataset into k subsets and using each subset as the validation set while the remaining subsets are used for training?**  
K-Fold cross-validation

---

**Which cross-validation technique involves training a model on all data except one instance, which is used for validation and is recommended for small datasets due to its computational intensity??**  
Leave-One-Out cross-validation

---

**Which technique involves selecting the best hyperparameters for a model by trying various combinations on a validation set?**  
Grid search

---

**In the context of hyperparameter tuning, what is "Random Search"?**  
Trying a random set of hyperparameters from a predefined search space

---

**What is the primary goal of using bagging in ensemble learning?**  
To reduce overfitting by training <mark>on subsets</mark> of the data

**Which ensemble learning technique involves training multiple models sequentially, where each model <mark>corrects the errors of the previous one</mark>?**  
<mark>Boosting</mark>

**What is the primary goal of using <mark>boosting</mark> in ensemble learning?**  
To sequentially correct <mark>errors of previous models</mark>

**Which ensemble learning technique involves training multiple decision trees and combining their predictions through <mark>voting?</mark>**  
Random Forest

**What is the primary advantage of using Random Forests in ensemble learning?**  
It prevents overfitting by training on subsets of the data

**Which technique is used to evaluate the stability and reliability of a machine learning model's predictions?**  
<mark>Bootstrap aggregation (bagging)</mark>

what "with replacement" means in **bagging**:

* **Without replacement**: If you pick a data point, it cannot be picked again. So, after picking all 100 points, you have exactly 100 unique points, no repeats.
    
* **<mark>With replacement</mark>**<mark>: </mark> After picking a data point, you can **put it back** and pick it again. This means you can randomly select the same point multiple times, resulting in some duplicates and some points not being picked at all.
    
    If you want to **test** how reliable and generalizable your model is on different data, use **k-fold cross-validation**.
    

If you want to **improve** the reliability and stability of your model's predictions, use **bagging** or **random forest** (for decision tree-based models).

**Which technique is used to create multiple datasets by randomly selecting instances with replacement from the original dataset?**  
Bootstrapping

**In the context of model evaluation, what is the purpose of creating <mark>a learning curve</mark>?**  
To depict the model's performance on the training and validation data over iterations

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729253898108/a7b46e61-e5f0-4bd4-af13-75ea56236d54.png align="center")

Here is the learning curve that represents **overfitting**. In this case, the training accuracy (orange line) continues to increase, approaching 100%, while the validation accuracy (red line) peaks early and then begins to decline. This indicates that the model is learning to fit the training data very well but struggles to generalize to new data, which is a key sign of overfitting

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729253926309/a9e2a1f3-dfc9-4e26-968b-31a187c59414.png align="center")

This graph represents **underfitting** where both the training and validation accuracy remain low throughout the training process. In this case, the model is too simple to capture the patterns in the data, resulting in poor performance for both the training and validation sets.

* **Training accuracy** stays low and improves slowly, indicating that the model isn't learning the underlying patterns well.
    
* **Validation accuracy** is also low and doesn't significantly diverge from training accuracy, reflecting that the model is not overfitting, but it's still unable to perform well on unseen data.
    

**Which technique is used to evaluate a machine learning model's performance on a range of hyperparameters?**  
Grid search

---

**Which metric is commonly used for evaluating binary classification models when class distribution is imbalanced?**  
ROC-AUC

**Dataset Example (Imbalanced):**

Imagine you have a dataset for detecting fraudulent transactions. Out of 1000 samples:

* 950 are legitimate transactions (class 0).
    
* 50 are fraudulent transactions (class 1).
    

This is a typical example of an imbalanced classification problem because the "fraudulent" class is much smaller than the "legitimate" class.

**Scenario: Before using ROC-AUC (using accuracy only)**

In imbalanced datasets, if you use **accuracy** as your main metric, it may not give a realistic picture of the model's performance. For example:

* The model could **predict everything as class 0** (legitimate) and get an accuracy of 95%, because it's correct for the majority class. This high accuracy can be **misleading**, as the model is **failing to detect any fraud cases (class 1)**.
    

**Output (before using ROC-AUC)**:

```python
Accuracy = 95% 
(Looks great, but the model is predicting every transaction as legitimate, ignoring fraudulent cases)
```

---

After ROC-AUC

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Generate an imbalanced dataset
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.95, 0.05], random_state=42)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit a RandomForest classifier
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Predict probabilities for the test set
y_pred_proba = model.predict_proba(X_test)[:, 1]

# Calculate ROC-AUC score
roc_auc = roc_auc_score(y_test, y_pred_proba)

# Plot ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
plt.plot(fpr, tpr, label=f'ROC-AUC = {roc_auc:.2f}')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

roc_auc
```

output

```python
0.8365977112676056
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729257973608/073ff966-262f-4304-8736-977cd98c5e07.png align="center")

In the example, we generated an imbalanced dataset with 950 legitimate transactions (class 0) and 50 fraudulent transactions (class 1). After training the Random Forest classifier and evaluating the ROC-AUC score, we obtained an ROC-AUC score of 0.84. This score indicates that the model is reasonably good at distinguishing between legitimate and fraudulent transactions, even though the dataset is imbalanced.

The ROC curve above shows the True Positive Rate (TPR) vs. the False Positive Rate (FPR) at various threshold settings. The ROC-AUC value of 0.84 indicates that the model has a high ability to discriminate between the two classes, with 1.0 being perfect and 0.5 being no better than random guessing.

---

**In K-means clustering, how is the number of clusters (K) determined?**  
It is determined using the Elbow Method.

Finding the Optimal Clusters: Elbow Method (Toy and Basket Analogy)

```python
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Sample data: represents points in 2D space
data = np.array([[1,1], [1,2], [2,1.5], 
                 [4,5], [5,6], [4,5.5], [5,5], 
                 [8,8], [8,8.5], [9,8], [8.5,9], [9,9]])

# Step 1: Find optimal k (number of clusters) using the Elbow Method
def find_best_k():
    sum_of_squared_distances = []
    K = range(1, 8)  # Testing for 1 to 7 clusters
    for k in K:
        km = KMeans(n_clusters=k)
        km = km.fit(data)
        sum_of_squared_distances.append(km.inertia_)
    
    # Plot the elbow plot to find the optimal k
    plt.plot(K, sum_of_squared_distances, 'bx-')
    plt.xlabel('k')
    plt.ylabel('Sum of Squared Distances')
    plt.title('Elbow Method for Optimal k')
    plt.show()

# Step 2: Run K-means clustering with the optimal number of clusters (k)
def run_kmeans(k, data):
    km = KMeans(n_clusters=k)
    km = km.fit(data)
    centroids = km.cluster_centers_  # Get the center of clusters
    return centroids

# Step 3: Plot the results showing the clusters and centroids
def plotresults():
    centroids = run_kmeans(3, data)  # Assuming k=3 as the optimal number
    # Plot points with different colors for each cluster
    plt.plot(data[0:3, 0], data[0:3, 1], 'ro', label='Cluster 1')  # Red points
    plt.plot(data[3:7, 0], data[3:7, 1], 'bo', label='Cluster 2')  # Blue points
    plt.plot(data[7:12, 0], data[7:12, 1], 'go', label='Cluster 3')  # Green points
    
    # Plot the centroids with black stars and label them
    for i in range(3):
        plt.plot(centroids[i, 0], centroids[i, 1], 'k*', markersize=15)
        plt.text(centroids[i, 0], centroids[i, 1], f"c{i}", fontsize=12)

    plt.legend()
    plt.show()

# Running the functions
find_best_k()  # Step 1: Find the best number of clusters
plotresults()  # Step 3: Plot the final clustering results
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729303346799/3dcb9e4b-9e73-47e2-a542-e98215aaf068.png align="center")

The point where the scatter stops decreasing sharply is the **elbow** — the ideal number of clusters, like having the right number of baskets for your toys.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729303357738/21fe1013-7d24-44bf-95f7-5c7a0139f750.png align="center")

**In K-means clustering, what is "inertia"?**  
The sum of squared distances between data points and their assigned cluster centers

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729303286349/43217781-0b49-4cd4-aeb7-0da438701a22.png align="center")

The output of the code produces a plot where the toys (data points) are colored based on the cluster (basket) they belong to. The black "X" marks represent the cluster centers.

The inertia value, which is the sum of squared distances between the data points and their respective cluster centers, is **15.98**. This indicates the level of scatter or how spread out the toys are within their assigned clusters. The lower the inertia, the closer the toys are to their respective centers

---

**Which clustering algorithm groups items based on how close they are to each other and can find clusters in any shape?**  
DBSCAN

The Problem Before DBSCAN:

* **Inflexibility of Shapes**: K-means only works well if clusters are roughly circular. It struggles with data where clusters have irregular or elongated shapes.
    
* **Handling Noise**: K-means also tries to assign every data point to a cluster, even if that point is an outlier or noise. This can lead to distorted clustering.
    

Why We Need Clusters in Any Shape:

* **Real-world data isn't always round**: Many natural clusters (like paths of animals, weather patterns, or groups of customers) come in all sorts of irregular shapes. We need algorithms that can identify these complex patterns accurately.
    
* **Handling Noise**: Not all points in data belong to meaningful clusters. DBSCAN can **ignore isolated points (noise)**, improving the accuracy of clustering for the core groups.
    

**In DBSCAN (Density-Based Spatial Clustering of Applications with Noise), what are the two primary parameters used to define a cluster?**  
Epsilon and minimum number of points

**What are DBSCAN Limitations?**

* DBSCAN’s results are **highly sensitive** to the choice of `eps` (the distance threshold) and `min_samples` (minimum points to form a cluster).
    
* **Challenge**: If `eps` is too small, you may get too many small clusters or lots of noise. If `eps` is too large, the clusters might merge together.
    
* **Finding the right** `eps` for different datasets can be tricky and often requires experimentation or domain knowledge.
    
* DBSCAN doesn’t scale as well as K-means for **large datasets**. It has higher computational complexity, especially for very large datasets, because it needs to check distances between many pairs of points.
    
* **K-means**: On the other hand, K-means can handle larger datasets more efficiently.
    
* DBSCAN is also very good at identfying outliers.
    

**What are Silhouette Score in DBSCAN?**

```python
from sklearn.datasets import make_blobs
from sklearn.cluster import DBSCAN
from sklearn import metrics
import matplotlib.pyplot as plt
import numpy as np

# Generate toy data (similar to previous example)
data, _ = make_blobs(n_samples=100, centers=3, cluster_std=0.6, random_state=42)

# Apply DBSCAN with eps=0.5 and min_samples=5
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(data)

# Calculate Silhouette Score
silhouette_score = metrics.silhouette_score(data, labels)

# Visualize the DBSCAN clustering results
unique_labels = set(labels)
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]

for label, color in zip(unique_labels, colors):
    if label == -1:
        # Noise points
        color = [0, 0, 0, 1]  # Black color for noise
    class_mask = (labels == label)
    plt.scatter(data[class_mask][:, 0], data[class_mask][:, 1], c=[color], label=f'Cluster {label}')

plt.title('DBSCAN Clustering (eps=0.5, min_samples=5)')
plt.legend()
plt.show()

# Display Silhouette Score
silhouette_score
```

Output

```python
silhouette_score= 0.671745051870598
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1729304767110/3c86ace4-f2ff-499c-acd6-6c66db8e601f.png align="center")

<mark>How to Read the Silhouette Score</mark> (0.67):

* **0.67** is a **good score**, indicating that the clusters are:
    
    * **Well-separated**: Most data points are closer to their own cluster than to the nearest neighboring cluster.
        
    * **Compact**: Points within the same cluster are grouped tightly.
        

Interpretation:

* **<mark>Close to 1</mark>**<mark>:</mark> A score closer to 1 (like 0.67 here) suggests that the points are very well clustered, <mark>with clear boundaries between clusters.</mark>
    
* **Close to 0**: A score near 0 would indicate that the clusters are overlapping, or that points are very close to the boundary between two clusters.
    

**Negative scores**: If the score were negative, it would suggest that points may have been misclassified into the wrong clusters.

---

**Which hierarchical clustering method starts with each data point as its own cluster and merges clusters based on their similarity?**  
Bottom-up (Agglomerative)

```python
          _________Books_________
         |                       |
     ____|____               _____|____
    |         |             |          |
Fiction    Non-Fiction   History    Biography
(A + B)   (C + D)
```

**What is the output of hierarchical clustering often represented using a tree-like diagram?**  
Dendrogram

* **<mark>Hierarchical (Agglomerative/Divisive)</mark>**<mark>: Looks like a </mark> **<mark>tree</mark>** with branches representing how clusters merge or split.
    
* **Non-Hierarchical (K-means/DBSCAN)**: Looks like distinct **groups** or **clusters** with no inherent hierarchical relationship.
    

---

**What is "algorithmic bias"?**  
Unfair discrimination resulting from systematic errors in algorithms. algorithmic bias is a combination of **both human factors (input and decisions)** and **systematic issues** within the algorithm.

**What is "individual fairness"?**  
**Individual fairness** in machine learning ensures that similar individuals receive similar outcomes. <mark> For example, in a job recommendation system, if Alice and Bob have the same qualifications, they should both receive similar job suggestions.</mark>

**Violations of individual fairness** can occur due to:

* **Bias in training data**: If historical biases (e.g., men being promoted more) exist, the model might recommend higher-level jobs to men, <mark>even if women have the same qualifications.</mark>
    
* **Feature weighting**: The model might unfairly weigh certain features (e.g., school attended) that disproportionately benefit one group.
    
* **Incomplete data**: <mark>If one person’s data is incomplete, the model might rank them unfairly, </mark> even with similar qualifications.
    
* **Proxy variables**: Variables like zip code, unintentionally linked to protected attributes <mark>(e.g., race or socio-economic status),</mark> can lead to biased outcomes.
    
* **Overfitting**: The model may learn irrelevant patterns that cause unfair distinctions between similar individuals.
    

To prevent this, fairness-aware algorithms and careful model adjustments are necessary during development. (Google’s What-If Tool,IBM's AI Fairness 360 (AIF360)

**What is "equal opportunity fairness"?**  
**Example**: In a loan approval model, if qualified applicants from different racial groups are approved at the same rate (true positive rate), the system demonstrates equal opportunity fairness

**What is "intersectional bias"?**  
Bias that occurs when multiple attributes intersect and lead to unique experiences. **Example**: A hiring algorithm might favor white men over Black women, even if both groups face challenges individually. <mark>The combination of being both Black and a woman leads to a distinct bias </mark> that neither group experiences on its own.

**What is "data anonymization"?**  
The process of making data anonymous and unlinkable to individuals. **Example**: A hospital shares health data for research. Before sharing, they anonymize the data by removing names, addresses, and other identifying details, ensuring researchers can't link the records back to specific patients

**What is "data minimization"?**  
Collecting only the data that is necessary for a specific purpose.

**What is "model transparency"?**  
The ability of a model to provide insights and reasoning for its predictions.

**Linear regression**: You can directly see how each feature impacts the outcome by checking feature coefficients.

* **Example**: In a salary prediction model, if the coefficient for "years of experience" is **2,000**, it means that for each additional year of experience, the predicted salary increases by **$2,000**.
    
* **Decision trees**: You can visualize the tree and follow the path from input features to prediction.
    

**What is "model explainability"?**  
The ability of a model to provide insights into its inner workings.

For complex models (e.g., deep learning, random forests), explainability techniques are used to clarify **how** decisions are made:

* **LIME/SHAP**: These are go-to tools for <mark>explaining black-box models</mark>. You use SHAP to get **feature importance** for each prediction, making it clear which features drove the decision.
    
* **Partial Dependence Plot (PDP)**: You use it to show how varying a single feature influences predictions globally.
    

What is "model interpretability"?  
The process of generating <mark>human-readable</mark> explanations for model predictions.

* **Counterfactual explanations**: You ask, “What needs to change in the input for a different outcome?” (e.g., if an applicant is rejected for a loan, which small change would result in approval?).
    
* **Decision trees** for interpretability when deployed models need to explain outcomes (e.g., how an individual feature leads to decisions).
    

**What is "algorithmic accountability"?**  
The responsibility of developers and organizations for the impact of AI systems.While there’s no specific **developer liability** law in the U.S. Many regions (like the EU's **GDPR**) require transparency and fairness in AI.

**What is "technological unemployment"?**  
The displacement of human workers due to technological advancements.

**What is "job polarization"?**  
The division of jobs into high-skilled and low-skilled categories, leaving few opportunities in the middle.

**What is "amplification of bias"?**  
For example, <mark>if a hiring algorithm is trained on past data where more men were hired for tech jobs, it might continue to favor male applicants</mark>, or a facial recognition system is trained on a dataset predominantly containing lighter-skinned individuals. As a result, the system might perform poorly in recognizing darker-skinned faces, amplifying racial bias.

**What is the "source domain" in transfer learning?**  
The domain where the pre-trained model was originally trained.  
Example: **Source Domain:** BERT is pre-trained on Wikipedia articles and BookCorpus (general text).**Target Domain:** You fine-tune the BERT model to classify customer reviews (target domain) as positive or negative.

**What is "domain adaptation" in transfer learning?**  
The process of adapting the model to a new domain with different data distributions. Example: A sentiment analysis model trained on Amazon reviews (source domain) can be adapted to evaluate sentiments in product development feedback (target domain),e<mark>ven though the data distribution and context are different.</mark>

**What is "meta-learning" in the context of AutoML?**  
in AutoML refers to the process of learning how different algorithms perform across various datasets, helping the system choose the best model for a new dataset. For example:

Pre-trained Meta-learning in AutoML:

* **Auto-Sklearn**: It uses a pre-built meta-learning database that includes performance data of various models on different datasets.
    
* **Auto-Sklearn** can be more accurate than traditional **sklearn** when it comes to selecting the best model for your dataset. This is because Auto-Sklearn uses meta-learning (pre-trained on various datasets) to recommend models that are likely to perform well based on past performance, saving you from manually experimenting with different algorithms.  
    

**What is "federated learning"?**  
**Data stays local**: In federated learning, the raw data (like personal texts, medical records) never leaves the user's device. <mark>Only model updates (e.g., weights or gradients) are shared with a central server,</mark> which means sensitive information remains private.

In contrast, Edge AI (**Inference only)**: Edge AI processes data locally on the device, which can improve privacy because the data doesn't need to be sent to the cloud for analysis. However, **Edge AI** primarily focuses o<mark>n running the model (inference), not training it,</mark> so it may still involve collecting data for future improvements.  

What is the main advantage of federated learning?  
It enables training on decentralized data without sharing raw data.

What is "server aggregation" in federated learning?  
The process of aggregating model updates on a central server.

**What is the main challenge of federated learning?**  
It requires dealing with data heterogeneity and communication constraints.  
  
**Data heterogeneity**: The data on each device can be very different (e.g., different users may have different behavior patterns or input formats), making it difficult to train a generalized model.

**Communication constraints**: Devices may have limited bandwidth, connectivity issues, or lower computational power, making it hard to synchronize updates and train efficiently.

What is "homomorphic encryption" in the context of federated learning?  
**Answer**: A technique for encrypting model updates before sending them to the server.

**What is "vertical federated learning"?**  
A type of federated learning where different features of data are distributed across devices.  
**Application**: Imagine two companies, one holding user purchasing data (e.g., transaction history) and another holding user demographic data (e.g., age, location). Using vertical federated learning, they can train a model together without sharing their raw data, by combining the feature sets to improve the model.

This approach is useful <mark>when different organizations have complementary data but want to maintain data privacy</mark>.

---