{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ooTEfOQA5US",
        "outputId": "efdaa2fc-df51-4c8d-9972-a185ed571491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "VpiSySkbIUUF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "z-Xs-PMEIq-L",
        "outputId": "ec806ff3-e297-4001-cf70-be4c2209b209"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1e0e33f8-dc6e-4b0c-89ea-84381f7679e2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1e0e33f8-dc6e-4b0c-89ea-84381f7679e2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Dataset.csv to Dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming the file name is 'Dataset.csv'\n",
        "df = pd.read_csv('Dataset.csv', encoding='latin1')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juasnATKI1FD",
        "outputId": "d9b3eb40-a4c1-4f82-9356-15a4966da30e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Label                                              Email\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Email Texts and Labels"
      ],
      "metadata": {
        "id": "Ry2Igi1jI_De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract 'texts' and 'labels'\n",
        "texts = df['Email'].tolist() #transform a column of a DataFrame into a Python list.\n",
        "labels = df['Label'].map({'ham': 0, 'spam': 1}).tolist() #replace values in a column based on a mapping dictionary\n",
        "\n",
        "# Print the total number of spam and ham emails\n",
        "print(\"Total no. of spam emails:\", sum(labels))\n",
        "print(\"Total no. of ham emails:\", len(labels) - sum(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unXGYJf6JFyM",
        "outputId": "ff7602c7-bd3c-4a53-a454-c91160840035"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total no. of spam emails: 747\n",
            "Total no. of ham emails: 4825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the Dataset"
      ],
      "metadata": {
        "id": "c0zPHurQMfnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training set: 70% of the original data\n",
        "\n",
        "Validation set: 15% of the original data\n",
        "\n",
        "Test set: 15% of the original data"
      ],
      "metadata": {
        "id": "4cEPHlMuMwoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "XzLtuepsMh6o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*  Here, 70% of the original dataset is assigned to the training set (X_train, y_train), because the test_size=0.3 means that 30% of the data is kept for the temporary set (X_temp, y_temp).\n",
        "*  Now, the temporary set (X_temp, y_temp), which contains 30% of the data, is split in half (test_size=0.5). This means 15% of the original data goes to the validation set (X_val, y_val) and the remaining 15% goes to the test set (X_test, y_test).\n",
        "* When you set random_state to a specific number (like 42), it ensures that every time you run the code, the splitting of data will always be the same.\n",
        "\n"
      ],
      "metadata": {
        "id": "xOc4kiQiNPyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Tokenize and Pad Sequences\n"
      ],
      "metadata": {
        "id": "KNl7WLzUPF-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Tokenizer object to process the text\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Teach the tokenizer the vocabulary from the training, validation, and test sets combined\n",
        "# This way, the tokenizer learns all the words used in the dataset\n",
        "tokenizer.fit_on_texts(X_train + X_val + X_test)\n",
        "\n",
        "# Convert the text in the training set to sequences of numbers (one number per word)\n",
        "# Each word is replaced by its corresponding number from the tokenizer's learned vocabulary\n",
        "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "# Do the same for the validation set\n",
        "sequences_val = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "# Do the same for the test set\n",
        "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Find the length of the longest sequence from all datasets (training, validation, and test)\n",
        "# This tells us how much padding we need so that all sequences have the same length\n",
        "max_sequence_length = max([len(seq) for seq in sequences_train + sequences_val + sequences_test])\n",
        "\n",
        "# Determine the total number of unique words in the vocabulary\n",
        "# Add 1 to include space for padding\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Pad the training sequences so they all have the same length (max_sequence_length)\n",
        "# If a sequence is shorter, it gets padded with zeros\n",
        "data_train = pad_sequences(sequences_train, maxlen=max_sequence_length)\n",
        "\n",
        "# Pad the validation sequences\n",
        "data_val = pad_sequences(sequences_val, maxlen=max_sequence_length)\n",
        "\n",
        "# Pad the test sequences\n",
        "data_test = pad_sequences(sequences_test, maxlen=max_sequence_length)\n",
        "\n"
      ],
      "metadata": {
        "id": "MsQFbG5IQi4u"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- **Tokenizer**: This is a tool that turns words into numbers. We first teach it the words from the dataset so it knows what words to expect.\n",
        "- **fit_on_texts()**: The tokenizer looks at the entire dataset (training, validation, and test) and builds a dictionary of all the unique words, so it doesn't miss any words.\n",
        "- **texts_to_sequences()**: After learning the words, the tokenizer can turn each word into a number. Words it knows will be replaced with their corresponding number. If the word is new, it gets skipped.\n",
        "- **max_sequence_length**: We find the longest sentence in the dataset so that we can pad all the shorter ones to this length. This way, all sentences will have the same number of words.\n",
        "- **vocab_size**: This is simply the number of unique words the tokenizer learned from the dataset, plus one extra spot for padding.\n",
        "- **pad_sequences()**: If a sentence is shorter than the longest one, we pad it with zeros to make it the same length. This is necessary because models like LSTMs need all input sentences to be of the same length to work properly.\n"
      ],
      "metadata": {
        "id": "w-mkpwmZQzQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a Word2Vec Model"
      ],
      "metadata": {
        "id": "cLNFAAeNT7XZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text from all datasets (training, validation, and test) into words\n",
        "# Each sentence is turned into a list of words by splitting on spaces\n",
        "# This gives us a list of sentences, where each sentence is a list of words\n",
        "sentences = [text.split() for text in X_train + X_val + X_test]\n",
        "\n",
        "# Train a Word2Vec model using the list of sentences\n",
        "# The model learns to represent each word as a vector in a 100-dimensional space (vector_size=100)\n",
        "# The \"window=5\" means the model looks at 5 words before and after the current word\n",
        "# Words that appear less than once (min_count=1) are ignored\n",
        "# The \"workers=4\" means we’ll use 4 CPU cores to make the training faster\n",
        "word2vec_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n"
      ],
      "metadata": {
        "id": "y15rSDdZT9A5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- **sentences=sentences**: We break down each sentence into individual words from the training, validation, and test sets. This creates a list where every sentence is a list of words.\n",
        "\n",
        "- **vector_size=100**: The model will learn to represent each word using a vector of 100 numbers, where each number represents a different feature of the word.\n",
        "\n",
        "- **window=5**: The model will consider 5 words before and 5 words after the current word to understand the word's context.\n",
        "\n",
        "- **min_count=1**: Even if a word appears only once, the model will still learn it. No words will be ignored based on their frequency.\n",
        "\n",
        "- **workers=4**: The model will use 4 CPU cores to train faster. The more cores, the quicker it will process the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "J1wENqbUVUlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the Embedding Matrix"
      ],
      "metadata": {
        "id": "jM6OHrpLUv9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty matrix to hold the word vectors\n",
        "# The matrix has 'vocab_size' rows (one for each word in the vocabulary)\n",
        "# Each row will hold a vector of size 'vector_size' (the dimensions of the word vectors)\n",
        "embedding_matrix = np.zeros((vocab_size, word2vec_model.vector_size))\n",
        "\n",
        "# Loop through all the words and their assigned indices in the tokenizer's word index\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    # Check if this word exists in the Word2Vec model's vocabulary\n",
        "    if word in word2vec_model.wv:\n",
        "        # If the word exists, place its word vector in the embedding matrix\n",
        "        # The word's vector is stored in the row that matches its index 'i'\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n"
      ],
      "metadata": {
        "id": "_lViCsDlWAEK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- **embedding_matrix = np.zeros((vocab_size, word2vec_model.vector_size))**:\n",
        "  - We create an empty matrix (full of zeros) to store the word vectors. Each word in the vocabulary will have its own row in this matrix. The number of columns is the size of the word vector (100 dimensions, as defined before).\n",
        "\n",
        "- **for word, i in tokenizer.word_index.items():**:\n",
        "  - This loop goes through each word and its index in the tokenizer's vocabulary. The index tells us which row in the embedding matrix corresponds to the word.\n",
        "\n",
        "- **if word in word2vec_model.wv:**:\n",
        "  - We check if this word exists in the Word2Vec model's learned vocabulary. If the word was seen during training, we’ll have its vector.\n",
        "\n",
        "- **embedding_matrix[i] = word2vec_model.wv[word]:**:\n",
        "  - If the word exists in the Word2Vec model, we place its vector in the corresponding row of the embedding matrix (at index 'i'). This way, each word gets represented by its vector in the matrix.\n"
      ],
      "metadata": {
        "id": "WzuaOQPUXHFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build an LSTM Model"
      ],
      "metadata": {
        "id": "TAZK3upIca_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build an LSTM model with Word2Vec embeddings\n",
        "model = Sequential()\n",
        "\n",
        "# Add an Embedding layer using the Word2Vec embeddings\n",
        "# vocab_size: number of words in the vocabulary\n",
        "# word2vec_model.vector_size: the size of the word vectors (100 dimensions)\n",
        "# weights=[embedding_matrix]: use the pre-trained word vectors from the embedding matrix\n",
        "# input_length: maximum length of the input sequences\n",
        "# trainable=False: freeze the embeddings during training so they don't change\n",
        "model.add(Embedding(vocab_size, word2vec_model.vector_size, weights=[embedding_matrix],\n",
        "                    input_length=max_sequence_length, trainable=False))\n",
        "\n",
        "# Add an LSTM layer with 100 units\n",
        "# dropout=0.2: drop 20% of the neurons to prevent overfitting\n",
        "# recurrent_dropout=0.2: apply dropout to the LSTM's recurrent connections\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# Add a Dense layer with 1 unit for binary classification (spam or not spam)\n",
        "# Use 'sigmoid' activation function, which outputs a value between 0 and 1\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Print the summary of the model to see its structure\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "zhLHUt3-cdu4",
        "outputId": "e5ef033c-57e9-417a-9941-76803aa45757"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │         \u001b[38;5;34m892,100\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">892,100</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m892,100\u001b[0m (3.40 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">892,100</span> (3.40 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m892,100\u001b[0m (3.40 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">892,100</span> (3.40 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- **Sequential()**: We’re building the model step by step using Keras's Sequential API, where we add one layer at a time.\n",
        "\n",
        "- **Embedding Layer**:\n",
        "  - We're using the word vectors we trained earlier with Word2Vec.\n",
        "  - The embedding layer takes the `vocab_size` (total words in the vocabulary) and `vector_size` (100 dimensions) to set up the word embeddings.\n",
        "  - **weights=[embedding_matrix]**: This uses the pre-trained Word2Vec embeddings.\n",
        "  - **trainable=False**: We freeze the word vectors so they don't get changed during training. This way, the model focuses on learning other things while keeping the word vectors fixed.\n",
        "\n",
        "- **LSTM Layer**:\n",
        "  - We use an LSTM layer with 100 units to process the sequence data.\n",
        "  - **dropout=0.2**: Dropout means we randomly drop 20% of the neurons during training to prevent overfitting.\n",
        "  - **recurrent_dropout=0.2**: Dropout is also applied to the recurrent connections within the LSTM to add more regularization.\n",
        "\n",
        "- **Dense Layer**:\n",
        "  - The final layer is a Dense layer with 1 unit, which is used for binary classification (spam or ham).\n",
        "  - **sigmoid activation**: The sigmoid function outputs a value between 0 and 1, making it perfect for binary classification tasks.\n",
        "\n",
        "- **model.summary()**: This gives a quick overview of the model architecture, showing the number of parameters and how each layer is connected.\n"
      ],
      "metadata": {
        "id": "yCVUW1GVcm6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile the Model"
      ],
      "metadata": {
        "id": "mc6Jz5EPc93r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model for training\n",
        "# loss='binary_crossentropy': This is the loss function used for binary classification (spam or not spam)\n",
        "# optimizer='adam': Adam optimizer is a good default choice for training neural networks\n",
        "# metrics=['accuracy']: We'll track accuracy during training to see how well the model is performing\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "JldiC55pdAHX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model"
      ],
      "metadata": {
        "id": "iOEsrH-Qdvm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with training data, labels, and validation data\n",
        "model.fit(data_train, np.array(y_train), epochs=10, batch_size=32, validation_data=(data_val, np.array(y_val)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyNIJN_jdw30",
        "outputId": "a1a74f2b-c8fd-4f2a-ed69-e3ce9fc92b47"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 136ms/step - accuracy: 0.8510 - loss: 0.3637 - val_accuracy: 0.9187 - val_loss: 0.2226\n",
            "Epoch 2/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 145ms/step - accuracy: 0.8954 - loss: 0.2619 - val_accuracy: 0.9007 - val_loss: 0.2304\n",
            "Epoch 3/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 138ms/step - accuracy: 0.9027 - loss: 0.2460 - val_accuracy: 0.9127 - val_loss: 0.2077\n",
            "Epoch 4/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 173ms/step - accuracy: 0.9130 - loss: 0.2298 - val_accuracy: 0.9163 - val_loss: 0.2001\n",
            "Epoch 5/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 135ms/step - accuracy: 0.9085 - loss: 0.2323 - val_accuracy: 0.9199 - val_loss: 0.1988\n",
            "Epoch 6/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 132ms/step - accuracy: 0.9060 - loss: 0.2292 - val_accuracy: 0.9246 - val_loss: 0.1956\n",
            "Epoch 7/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 134ms/step - accuracy: 0.9083 - loss: 0.2227 - val_accuracy: 0.9127 - val_loss: 0.1897\n",
            "Epoch 8/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 145ms/step - accuracy: 0.9140 - loss: 0.2274 - val_accuracy: 0.9187 - val_loss: 0.1892\n",
            "Epoch 9/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 137ms/step - accuracy: 0.9092 - loss: 0.2239 - val_accuracy: 0.9246 - val_loss: 0.1867\n",
            "Epoch 10/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 154ms/step - accuracy: 0.9194 - loss: 0.2042 - val_accuracy: 0.9187 - val_loss: 0.1853\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7943803f89a0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- **Training process**: We're training the model for 10 cycles (epochs), meaning the model will see the full training data 10 times to learn the patterns.\n",
        "  \n",
        "- **Batch size of 32**: The model processes 32 samples at a time before updating its weights, which helps make training faster and more stable.\n",
        "\n",
        "- **Validation data**: This is used to see how well the model performs on data it hasn't trained on, which helps us understand whether the model is overfitting or generalizing well.\n"
      ],
      "metadata": {
        "id": "ol4GKWJEg1MY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Evaluate the Model\n"
      ],
      "metadata": {
        "id": "xFnq9yO_hFJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model's performance on the test set\n",
        "evaluation_results = model.evaluate(data_test, np.array(y_test))\n",
        "\n",
        "# Display the test loss and accuracy\n",
        "print(\"Test Loss:\", evaluation_results[0])\n",
        "print(\"Test Accuracy:\", evaluation_results[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwS7vvPHhKGM",
        "outputId": "c8002c14-7966-44f7-cc86-cdee1c2cc9f3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9069 - loss: 0.2287\n",
            "Test Loss: 0.23843520879745483\n",
            "Test Accuracy: 0.9019138813018799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- **Evaluating the model**: This step checks how well the model performs on completely unseen data (the test set). We’re using the test data and its labels to calculate two key metrics:\n",
        "  - **Test loss**: How far off the model's predictions are from the true labels.\n",
        "  - **Test accuracy**: The percentage of correct predictions made by the model on the test set.\n",
        "  \n",
        "- **Why it's important**: Evaluating the model on the test set gives us a sense of how well it will perform in real-world scenarios, where the data will be similar to the test set.\n",
        "### Interpretation:\n",
        "\n",
        "- **Test Loss: 0.2384**: This means the average difference between the predicted labels and the actual labels on the test set is relatively small. Lower loss values indicate that the model's predictions are more accurate.\n",
        "\n",
        "- **Test Accuracy: 90.19%**: The model correctly classified about 90% of the emails in the test set. This is a solid performance, indicating that the model generalizes well to unseen data and could be effective for classifying emails as spam or not spam.\n",
        "\n",
        "### What it means:\n",
        "- **Good performance**: A test accuracy of 90% means the model is reliable and likely to perform well in real-world scenarios where it has to classify emails it hasn’t seen before.\n",
        "- **Room for improvement**: While 90% accuracy is strong, you can still explore improvements by tuning the model, experimenting with more data, or trying different architectures.\n",
        "\n",
        "In summary, the model does a good job of classifying emails, with a reasonably low error (loss) and a high accuracy.\n"
      ],
      "metadata": {
        "id": "zjoa_RtxhvMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Predictions"
      ],
      "metadata": {
        "id": "1VIgTQ3si0A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predicted probabilities for the test set\n",
        "predictions = model.predict(data_test)\n",
        "\n",
        "# Convert the probabilities into binary predictions\n",
        "# If the probability is greater than 0.5, classify as 1 (spam); otherwise, classify as 0 (not spam)\n",
        "predictions = (predictions > 0.5).astype(int)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3zaYfGji5Du",
        "outputId": "87075fed-e7f5-4016-b90f-4bb53d91b16f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **predictions = model.predict(data_test)**: The model generates probabilities for each email in the test set. These probabilities represent the likelihood that an email is spam. A value close to 1 means it's likely spam, while a value close to 0 means it's likely not spam.\n",
        "\n",
        "- **predictions = (predictions > 0.5).astype(int)**: We convert these probabilities into binary predictions (spam or not spam) by setting a threshold at 0.5. If the probability is greater than 0.5, we classify the email as spam (1). Otherwise, it's classified as not spam (0).\n"
      ],
      "metadata": {
        "id": "DK7t8H-gjCnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print the Classification Report"
      ],
      "metadata": {
        "id": "MEzCu7jQjSwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(np.array(y_test), predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQTzFXe0jT0Y",
        "outputId": "5fb8208d-2340-4c18-f014-2195ee317b40"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.97      0.94       724\n",
            "           1       0.71      0.46      0.55       112\n",
            "\n",
            "    accuracy                           0.90       836\n",
            "   macro avg       0.81      0.71      0.75       836\n",
            "weighted avg       0.89      0.90      0.89       836\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- **classification_report()**: This function gives a detailed summary of how well the model is performing. It shows key metrics like:\n",
        "  - **Precision**: Out of all the emails predicted as spam, how many were actually spam?\n",
        "  - **Recall**: Out of all the actual spam emails, how many did the model correctly identify as spam?\n",
        "  - **F1-Score**: A balance between precision and recall, useful for understanding the model’s overall performance.\n",
        "  - **Accuracy**: The overall percentage of correct predictions (both spam and not spam).\n",
        "### Interpretation:\n",
        "\n",
        "- **Class 0 (Not Spam)**:\n",
        "  - **Precision (0.92)**: Out of all the emails predicted as **not spam**, 92% were actually **not spam**.\n",
        "  - **Recall (0.97)**: Out of all the emails that were actually **not spam**, the model correctly identified 97% of them.\n",
        "  - **F1-score (0.94)**: This is a balance between precision and recall, showing that the model performs very well in identifying emails that are not spam.\n",
        "\n",
        "- **Class 1 (Spam)**:\n",
        "  - **Precision (0.71)**: Out of all the emails predicted as **spam**, 71% were actually **spam**.\n",
        "  - **Recall (0.46)**: Out of all the actual **spam** emails, the model correctly identified only 46%. This indicates the model struggles with catching all the spam emails.\n",
        "  - **F1-score (0.55)**: This indicates that the model is less effective at identifying spam, with a noticeable gap between precision and recall.\n",
        "\n",
        "- **Accuracy (0.90)**: Overall, the model correctly classified 90% of the emails, both spam and not spam.\n",
        "\n",
        "- **Macro avg** (Average of Class 0 and Class 1):\n",
        "  - **Precision (0.81)**: On average, the model is 81% precise in its predictions across both spam and not spam.\n",
        "  - **Recall (0.71)**: On average, the model correctly identifies 71% of the actual labels across both classes.\n",
        "  - **F1-score (0.75)**: This is the overall balance between precision and recall across both classes.\n",
        "\n",
        "- **Weighted avg** (Weighted by the number of samples in each class):\n",
        "  - The weighted average takes into account that there are many more **not spam** emails (724) than **spam** emails (112), giving an overall performance score of the model as:\n",
        "    - **Precision (0.89)**, **Recall (0.90)**, and **F1-score (0.89)**.\n",
        "\n",
        "### Summary:\n",
        "- The model is **very good** at correctly identifying **not spam** emails but **struggles** with identifying **spam** emails.\n",
        "- While it performs well overall with 90% accuracy, the lower recall for spam (46%) suggests that the model is missing a significant portion of actual spam emails. This means some spam might still get through undetected.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rAaSYDZRjkQN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y-y_vR3dkFBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classify a new email"
      ],
      "metadata": {
        "id": "fMeEvSXDkfxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example email to classify\n",
        "new_email = [\"Congratulations! You've won a free gift card. Click here to claim it!\"]\n",
        "\n",
        "# Preprocess the new email (tokenize and pad the sequence)\n",
        "new_email_sequence = tokenizer.texts_to_sequences(new_email)\n",
        "new_email_padded = pad_sequences(new_email_sequence, maxlen=max_sequence_length)\n",
        "\n",
        "# Make a prediction\n",
        "prediction = model.predict(new_email_padded)\n",
        "\n",
        "# Interpret the prediction\n",
        "if prediction >= 0.5:\n",
        "    print(\"The email is classified as SPAM.\")\n",
        "else:\n",
        "    print(\"The email is classified as NOT SPAM.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giFlfhPVkjR_",
        "outputId": "27a2692b-3cac-426e-ddf9-597c96897ae3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "The email is classified as NOT SPAM.\n"
          ]
        }
      ]
    }
  ]
}