---
title: "Prompt Engineering Essentials: LangChain's Prompt, Few-Shot, and Output Parsers"
datePublished: Tue Nov 12 2024 07:39:14 GMT+0000 (Coordinated Universal Time)
cuid: cm3e55u88000109ml9rgz1b8k
slug: prompt-engineering-essentials-langchains-prompt-few-shot-and-output-parsers
tags: prompt, llm, promptengineering, langchain, few-shot-learning

---

Prompt engineering centers around designing the input (the "prompt") to make large language models (LLMs) produce the most useful, accurate, and tailored responses. In essence, prompt engineering includes:

1. **Prompt Templates**: Defining a "template" for a prompt where you can fill in variables to make it flexible and reusable.
    
2. **Few-Shot Learning and Example Selectors**: Providing specific examples or guidelines so the model "learns" the expected response style or format, which helps with tasks like summaries, classifications, or creative responses.
    
3. **Output Parsers**: Formatting the model's output (e.g., CSV, JSON) so it’s easier to use in downstream applications.
    

Using tools like LangChain can streamline this process, especially for generating prompts dynamically, creating more complex interactions, and refining outputs for real-world applications.

So yes—while prompt engineering might look simple on the surface, the layers that LangChain introduces (templates, selectors, parsers) are powerful because they give you much finer control over LLM interactions, transforming them from simple text generators into customizable, purpose-driven engines.

---

# 1\. **Prompt Templates Module**

This module allows us to create a prompt structure with placeholders, making it easy to reuse the prompt by just filling in the blanks.

**Code Example:**

```python
from langchain.prompts import PromptTemplate

template = "I love traveling! I've visited {places}. Can you summarize my trips in {word_count} words?"
prompt = PromptTemplate(input_variables=["places", "word_count"], template=template)

formatted_prompt = prompt.format(places="10 countries", word_count="15")
print("Prompt sent to LLM:", formatted_prompt)
```

**Sample Output**:

```python
Prompt sent to LLM: I love traveling! I've visited 10 countries. Can you summarize my trips in 15 words?
```

Here, instead of rewriting the whole prompt each time, you can simply change `places` and `word_count` values, which is particularly useful for interactive applications.

---

# 2\. **Few-shot Learning with Example Selectors**

Few-shot learning allows us to provide the model with specific examples, which helps it better understand the task or context. This is especially useful for generating responses with a particular tone, style, or structure.

**Code Example:**

```python
from langchain.prompts import FewShotPromptTemplate

# Define example prompts
examples = [
    {"query": "What is a cat?", "answer": "A cute, furry pet that says 'meow'."},
    {"query": "What is a dog?", "answer": "A loyal animal that loves to bark and play."}
]

# Define the template for each example
example_prompt = PromptTemplate(input_variables=["query", "answer"], template="Q: {query}\nA: {answer}\n")

# Define the few-shot prompt
few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="Answer like a five-year-old:",
    suffix="Q: {query}\nA:",
    input_variables=["query"]
)

formatted_prompt = few_shot_prompt.format(query="What is a house?")
print("Prompt sent to LLM:\n", formatted_prompt)
```

* Here, we define `example_prompt` as a `PromptTemplate`, specifying how each example should be formatted.
    
* The `input_variables` field lists the placeholders we’ll use (`query` and `answer`).
    
* The `template` string is the format for each example, where each query and answer pair appears as
    
* **Parameters**:
    
    * `examples`: The examples we created earlier.
        
    * `example_prompt`: The format we defined for each example.
        
    * `prefix`: A message placed at the beginning of the prompt to set context, which in this case tells the model to answer like a five-year-old.
        
    * `suffix`: Text added at the end of the prompt that includes the new question, using `{query}` as a placeholder.
        
    * `input_variables`: Specifies the variable (`query`) that will be replaced in the suffix when formatting new questions.
        
          
        **Sample Output**:
        

```python
Prompt sent to LLM:
Answer like a five-year-old:
Q: What is a cat?
A: A cute, furry pet that says 'meow'.

Q: What is a dog?
A: A loyal animal that loves to bark and play.

Q: What is a house?
AA:
```

In this example, by showing the model how a “five-year-old” would answer questions, it’s more likely to respond to "What is a house?" in a similar tone, potentially outputting something like:

```python
A house is a cozy place where my family lives and we have fun together.
```

---

# 3\. **Output Parsers**

Output parsers allow you to specify the structure of the response you want, such as in JSON format or as a comma-separated list. This is helpful when the output needs to be used in a particular application or parsed further.

#### JSON Output Parser Example

**Code Example:**

```python
from langchain.output_parsers import StructuredOutputParser, ResponseSchema

# Define the output structure
schema = [
    ResponseSchema(name="currency", description="The name of the currency"),
    ResponseSchema(name="abbreviation", description="The abbreviation of the currency")
]
parser = StructuredOutputParser.from_response_schemas(schema)

# Prompt template including format instructions
template = "What is the currency of {country}?\n{format_instructions}"
prompt = PromptTemplate(
    input_variables=["country"],
    template=template,
    partial_variables={"format_instructions": parser.get_format_instructions()}
)

# Generate the prompt
formatted_prompt = prompt.format(country="Japan")
print("Prompt sent to LLM:\n", formatted_prompt)
```

**Sample Output**:

```python
Prompt sent to LLM:
What is the currency of Japan?
Your response should be in the following JSON format:
{
    "currency": "The currency name",
    "abbreviation": "The currency abbreviation"
}
```

The LLM might then respond with:

```json
{
    "currency": "Yen",
    "abbreviation": "JPY"
}
```

With this setup, you can extract specific information directly in JSON format, making it easy to handle structured data.

---

The `StructuredOutputParser` and `ResponseSchema` in LangChain's `output_parsers` module are tools that help format and structure the responses generated by a language model, like ensuring they follow a specified format (such as JSON) that’s easier to process programmatically.

### 1\. `ResponseSchema`

* **Purpose**: Defines individual parts of the response format you want.
    
* **Use**: You create a `ResponseSchema` for each item in the output you’re expecting.
    
* **Components**:
    
    * `name`: The key or label you want in the output (e.g., `"currency"` or `"abbreviation"`).
        
    * `description`: A brief description of what data should be under this key, helping guide the model.
        

```python
schema = ResponseSchema(name="currency", description="The name of the currency")
```

* **Example**: If you have a schema with `name="currency"` and `description="The name of the currency"`, this defines that the output should include a `currency` field that provides the name of a currency, e.g., `"Japanese Yen"`.
    

### 2\. `StructuredOutputParser`

* **Purpose**: Combines multiple `ResponseSchema` definitions to create a structured output template that the model should follow.
    
* **Use**: It generates format instructions for the model, guiding it to respond with a JSON-like structure (or another structured format you define).
    
* **How It Works**: The `StructuredOutputParser` takes in a list of `ResponseSchema` objects and uses them to create detailed instructions for the model, specifying the structure and content required.
    

```python
parser = StructuredOutputParser.from_response_schemas(schema)
```

* **Generated Instructions**: When you call `parser.get_format_instructions()`, it returns instructions like:
    
    ```python
    Your response should be in the following JSON format: {"currency": "", "abbreviation": ""}
    ```
    
    This message makes it clear to the model to structure its output as specified.
    

### How They Work Together

In summary:

* `ResponseSchema` defines each part of the response individually.
    
* `StructuredOutputParser` combines these parts into a cohesive format and generates instructions, helping the language model return structured, predictable data.
    

This setup is especially useful when you need the model’s output to feed directly into other automated systems, as it guarantees consistency in how the data is structured.

Here's a step-by-step explanation of this code:

### 1\. **Import Necessary Classes**

```python
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
```

* `StructuredOutputParser` and `ResponseSchema` are imported from LangChain's `output_parsers` module. These help format the language model's response into a structured output that follows a specified format (e.g., JSON).
    

### 2\. **Define the Output Structure**

```python
schema = [
    ResponseSchema(name="currency", description="The name of the currency"),
    ResponseSchema(name="abbreviation", description="The abbreviation of the currency")
]
```

* Here, we define the output format we want the language model to follow. This is done by creating a `schema` list that includes two `ResponseSchema` objects.
    
* Each `ResponseSchema` object has:
    
    * `name`: The key that we want in the response (e.g., `"currency"` or `"abbreviation"`).
        
    * `description`: A description of what data each key should contain, helping the model understand how to structure its response.
        
* For example, the output should look something like:
    
    ```json
    {
      "currency": "Japanese Yen",
      "abbreviation": "JPY"
    }
    ```
    

### 3\. **Create the Structured Output Parser**

```python
parser = StructuredOutputParser.from_response_schemas(schema)
```

* The `StructuredOutputParser.from_response_schemas` method takes our `schema` as input and creates a parser object.
    
* This parser will generate instructions for the language model on how to format its response. Specifically, it tells the model to provide a JSON response matching the schema we defined.
    

### 4\. **Define the Prompt Template**

```python
template = "What is the currency of {country}?\n{format_instructions}"
```

* This template defines the structure of the question we’ll ask the model. It includes two placeholders:
    
    * `{country}`: This will be replaced with the specific country we’re inquiring about.
        
    * `{format_instructions}`: This will be filled with the output format instructions from the parser, guiding the model on how to structure its response.
        

### 5\. **Create the PromptTemplate Object**

```python
prompt = PromptTemplate(
    input_variables=["country"],
    template=template,
    partial_variables={"format_instructions": parser.get_format_instructions()}
)
```

* Here, we create a `PromptTemplate` object that will build the final prompt to send to the model.
    
* **Parameters**:
    
    * `input_variables`: Specifies the placeholders we’ll replace, here just `"country"`.
        
    * `template`: Uses the `template` defined in the previous step.
        
    * `partial_variables`: This is where we replace `{format_instructions}` with the actual instructions generated by the `parser`.
        
        * `parser.get_format_instructions()` returns instructions like:
            
            ```python
            Your response should be in the following JSON format: {"currency": "", "abbreviation": ""}
            ```
            
        * This makes it clear to the model what structure to follow in its response.
            

### 6\. **Generate the Formatted Prompt**

```python
formatted_prompt = prompt.format(country="Japan")
```

* This formats the prompt by replacing `{country}` with `"Japan"`, while `{format_instructions}` is already filled in from `partial_variables`.
    
* **Resulting Prompt**:
    
    ```python
    What is the currency of Japan?
    Your response should be in the following JSON format: {"currency": "", "abbreviation": ""}
    ```
    

### 7\. **Display the Prompt**

```python
print("Prompt sent to LLM:\n", formatted_prompt)
```

* This prints out the complete prompt we’ll send to the model, showing both the question and the required response format.
    

### Summary of the Final Prompt

This process creates a structured prompt that asks, "What is the currency of Japan?" and instructs the language model to respond in JSON format:

```json
{
  "currency": "Japanese Yen",
  "abbreviation": "JPY"
}
```

This structure makes it easier for automated systems to use and validate the model’s response.